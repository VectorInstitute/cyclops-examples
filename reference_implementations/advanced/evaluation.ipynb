{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6y9DI1XXF2y"
   },
   "source": [
    "# Excercise 03 - Evaluating the trained readmission model\n",
    "\n",
    "Welcome to the third hands-on excercise to get started with CyclOps!\n",
    "\n",
    "We will use a trained model, evaluate it across different patient subpopulations and across various metrics. At the end of this excercise, you will be able to:\n",
    "\n",
    "1. Run inference using a trained ML model, to generate predictions on a test dataset\n",
    "2. Evaluate the model on the test set across across different sub-groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpK5KHfAXF29",
    "tags": []
   },
   "source": [
    "## Step 01 - Install CyclOps\n",
    "\n",
    "CyclOps is available as a [python package](https://pypi.org/project/pycyclops/) and can be installed using ``pip``. Note that we now install ``CyclOps`` with and extra dependency ``xgboost`` since we will be using the [xgboost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) library.\n",
    "\n",
    "``Colab`` would ask you to restart the session, which is normal. Click on ``Restart Session`` and re-run the cell to install ``CyclOps``.\n",
    "\n",
    "**NOTE**: We uninstall ``cupy`` from the colab runtime to avoid conflicts with ``CyclOps`` which would attempt to use ``cupy`` if it is installed. Since the runtime does not support GPUs, we will uninstall ``cupy``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwS7bAZAXF29",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall cupy-cuda12x -y\n",
    "!pip install 'pycyclops[xgboost]'\n",
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBYJTBGGXF2-"
   },
   "source": [
    "## Step 02 - Load and prepare the [Diabetes 130-US Hospitals for Years 1999-2008](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) dataset\n",
    "\n",
    "We already explored the [Diabetes 130-US Hospitals for Years 1999-2008](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) dataset. Now, we will select a subset from the dataset, and process it so that we can create training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wr_GUXoQXF2-"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets.features import ClassLabel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from cyclops.data.df.feature import TabularFeatures\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASKuJe2lXF2_"
   },
   "outputs": [],
   "source": [
    "diabetes_130_data = fetch_ucirepo(\n",
    "    id=296\n",
    ")  # This ID specifically corresponds to the Diabetes 130 dataset\n",
    "features = diabetes_130_data[\"data\"][\"features\"]\n",
    "targets = diabetes_130_data[\"data\"][\"targets\"]\n",
    "metadata = diabetes_130_data[\"metadata\"]\n",
    "variables = diabetes_130_data[\"variables\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC_aihbTXF2_"
   },
   "source": [
    "We will transform the readmitted variable into binary 0/1 labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfB6GmMbXF2_"
   },
   "outputs": [],
   "source": [
    "def transform_label(value):\n",
    "    \"\"\"Transform string labels of readmission into 0/1 binary labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value: str\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 if not readmitted or if greater than 30 days, 1 if less than 30 days\n",
    "\n",
    "    \"\"\"\n",
    "    if value in [\"NO\", \">30\"]:\n",
    "        return 0\n",
    "    if value == \"<30\":\n",
    "        return 1\n",
    "\n",
    "    raise ValueError(\"Unexpected value for readmission!\")\n",
    "\n",
    "\n",
    "df = features\n",
    "targets.loc[:, \"readmitted\"] = targets[\"readmitted\"].apply(transform_label)\n",
    "df.loc[:, \"readmitted\"] = targets[\"readmitted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKRKIbgaXF2_"
   },
   "source": [
    "Due to the large size of the dataset (around 100k examples), we will choose a small subset for training an ML model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXcCLxyKXF2_"
   },
   "outputs": [],
   "source": [
    "df = df[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert NaNs in the ``race`` column to a ``Unknown`` category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"race\"]] = df[[\"race\"]].fillna(value=\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GO5c-JCuXF2_"
   },
   "source": [
    "We previously looked at the missingness in the data. Let's remove features that are NaNs or have just a single unique value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj0_Uo2ZXF3A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_remove = []\n",
    "for col in df:\n",
    "    if len(df[col].value_counts()) <= 1:\n",
    "        features_to_remove.append(col)\n",
    "df = df.drop(columns=features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hULher3oXF3A"
   },
   "source": [
    "It is also important that we understand the class imbalance and use it to train our binary classifier to weight the class with fewer examples accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFlCdjCDXF3A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_counts = df[\"readmitted\"].value_counts()\n",
    "class_ratio = class_counts[0] / class_counts[1]\n",
    "print(class_ratio, class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcD-7U3xXF3A"
   },
   "source": [
    "From the features in the dataset, we select all of them except the label to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YxHeVFIXF3B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_list = list(df.columns)\n",
    "features_list.remove(\"readmitted\")\n",
    "features_list = sorted(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MQI59OZXF3B"
   },
   "source": [
    "### Identifying feature types\n",
    "\n",
    "Cyclops `TabularFeatures` class helps to identify feature types, an essential step before preprocessing the data. Understanding feature types (numerical/categorical/binary) allows us to apply appropriate preprocessing steps for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q9SObqQXF3B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tab_features = TabularFeatures(\n",
    "    data=df.reset_index(),\n",
    "    features=features_list,\n",
    "    by=\"index\",\n",
    "    targets=\"readmitted\",\n",
    ")\n",
    "print(tab_features.types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnt4Hf6XXF3B"
   },
   "source": [
    "### Creating data preprocessors\n",
    "\n",
    "We create a data preprocessor using sklearn's ColumnTransformer. This helps in applying different preprocessing steps to different columns in the dataframe. For instance, binary features might be processed differently from numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YQwpWVmXF3B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())],\n",
    ")\n",
    "\n",
    "binary_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXAoI2f-XF3B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_features = sorted((tab_features.features_by_type(\"numeric\")))\n",
    "numeric_indices = [\n",
    "    df[features_list].columns.get_loc(column) for column in numeric_features\n",
    "]\n",
    "print(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8U1tRahXF3B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_features = sorted(tab_features.features_by_type(\"binary\"))\n",
    "binary_features.remove(\"readmitted\")\n",
    "ordinal_features = sorted(\n",
    "    tab_features.features_by_type(\"ordinal\")\n",
    "    + [\"medical_specialty\", \"diag_1\", \"diag_2\", \"diag_3\"]\n",
    ")\n",
    "binary_indices = [\n",
    "    df[features_list].columns.get_loc(column) for column in binary_features\n",
    "]\n",
    "ordinal_indices = [\n",
    "    df[features_list].columns.get_loc(column) for column in ordinal_features\n",
    "]\n",
    "print(binary_features, ordinal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSuXI5inXF3C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_indices),\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            binary_indices + ordinal_indices,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Urrs9fN-XF3C"
   },
   "source": [
    "### Creating Hugging Face Dataset\n",
    "\n",
    "We convert our processed Pandas dataframe into a Hugging Face dataset, a powerful and easy-to-use data format which is also compatible with CyclOps modules. The dataset is then split to train and test sets (80:20 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NE_E1ZuWXF3A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 85\n",
    "TRAIN_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-uJcUElXF3C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset.cleanup_cache_files()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAmfkO4TXF3C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"readmitted\", ClassLabel(num_classes=2))\n",
    "dataset = dataset.train_test_split(\n",
    "    train_size=TRAIN_SIZE,\n",
    "    stratify_by_column=\"readmitted\",\n",
    "    seed=RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aoQG1c9XF3C"
   },
   "source": [
    "## Step 03 - Create model and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "993EDShYXF3C"
   },
   "source": [
    "### Model Creation\n",
    "\n",
    "CyclOps model registry allows for straightforward creation and selection of models. This registry maintains a list of pre-configured models, which can be instantiated with a single line of code. Here we use a [XGBoost classifier](https://xgboost.readthedocs.io/en/stable/python/python_api.html) to fit a binary classification model. The model configurations can be passed to `create_model` based on the parameters for the ``XGBClassifier``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUOYF98OXF3C"
   },
   "outputs": [],
   "source": [
    "from cyclops.models.catalog import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBMCH_UeXF3C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"xgb_classifier\"\n",
    "model = create_model(model_name, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhX-ElCEXF3C"
   },
   "source": [
    "### Task Creation\n",
    "\n",
    "We use Cyclops tasks to define our model's task (in this case, readmission prediction), train the model, make predictions, and evaluate performance. Cyclops task classes encapsulate the entire ML pipeline into a single, cohesive structure, making the process smooth and easy to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amWcdE4efJZQ"
   },
   "outputs": [],
   "source": [
    "from cyclops.tasks import BinaryTabularClassificationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kryn7LwXF3C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "readmission_prediction_task = BinaryTabularClassificationTask(\n",
    "    {model_name: model},\n",
    "    task_features=features_list,\n",
    "    task_target=\"readmitted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71RnIxN0XF3D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "readmission_prediction_task.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm6owwERXF3D"
   },
   "source": [
    "### Training\n",
    "\n",
    "If `best_model_params` is passed to the `train` method, the best model will be selected after the hyperparameter search. The parameters in `best_model_params` indicate the values to create the parameters grid.\n",
    "\n",
    "Note that the data preprocessor needs to be passed to the tasks methods if the Hugging Face dataset is not already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FAOWDPHXF3D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_params = {\n",
    "    \"n_estimators\": [250, 500],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"max_depth\": [5],\n",
    "    \"reg_lambda\": [0, 1, 10],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"gamma\": [0, 1],\n",
    "    \"method\": \"random\",\n",
    "    \"scale_pos_weight\": [int(class_ratio)],\n",
    "}\n",
    "readmission_prediction_task.train(\n",
    "    dataset[\"train\"],\n",
    "    model_name=model_name,\n",
    "    transforms=preprocessor,\n",
    "    best_model_params=best_model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 04 - Evaluate the model across different sub-groups of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate predictions over the test set. The prediction output can be either the whole HuggingFace dataset with the prediction columns added to it or the single column containing the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = readmission_prediction_task.predict(\n",
    "    dataset[\"test\"],\n",
    "    model_name=model_name,\n",
    "    transforms=preprocessor,\n",
    "    proba=True,\n",
    "    only_predictions=True,\n",
    ")\n",
    "prediction_df = pd.DataFrame(\n",
    "    {\n",
    "        \"y_prob\": [y_pred_i[1] for y_pred_i in y_pred],\n",
    "        \"y_true\": dataset[\"test\"][\"readmitted\"],\n",
    "        \"gender\": dataset[\"test\"][\"gender\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation is done using various evaluation metrics that provide different perspectives on the model's predictive abilities.\n",
    "\n",
    "The standard performance metrics can be created using the ``MetricDict`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclops.data.slicer import SliceSpec\n",
    "from cyclops.evaluate.metrics import create_metric\n",
    "from cyclops.evaluate.metrics.experimental.metric_dict import MetricDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [\n",
    "    \"binary_accuracy\",\n",
    "    \"binary_precision\",\n",
    "    \"binary_recall\",\n",
    "    \"binary_f1_score\",\n",
    "    \"binary_auroc\",\n",
    "    \"binary_average_precision\",\n",
    "    \"binary_roc_curve\",\n",
    "    \"binary_precision_recall_curve\",\n",
    "]\n",
    "metrics = [\n",
    "    create_metric(metric_name, experimental=True) for metric_name in metric_names\n",
    "]\n",
    "metrics = MetricDict(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MetricDict can also be defined for the fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = create_metric(metric_name=\"binary_specificity\", experimental=True)\n",
    "sensitivity = create_metric(metric_name=\"binary_sensitivity\", experimental=True)\n",
    "\n",
    "fpr = -specificity + 1\n",
    "fnr = -sensitivity + 1\n",
    "\n",
    "ber = (fpr + fnr) / 2\n",
    "\n",
    "fairness_metric_collection = MetricDict(\n",
    "    {\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"BER\": ber,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FairnessConfig helps in setting up and evaluating the fairness of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclops.evaluate.fairness import FairnessConfig\n",
    "\n",
    "fairness_config = FairnessConfig(\n",
    "    metrics=fairness_metric_collection,\n",
    "    dataset=None,  # dataset is passed from the evaluator\n",
    "    target_columns=None,  # target columns are passed from the evaluator\n",
    "    groups=[\"race\"],\n",
    "    group_base_values={\"race\": \"Caucasian\"},\n",
    "    thresholds=[0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to overall metrics, it might be interesting to see how the model performs on certain subpopulations. We can define these subpopulations using ``SliceSpec`` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_list = [\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"value\": \"[60-70)\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"age\": {\n",
    "            \"value\": \"[70-80)\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "slice_spec = SliceSpec(spec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, dataset_with_preds = readmission_prediction_task.evaluate(\n",
    "    dataset=dataset[\"test\"],\n",
    "    metrics=metrics,\n",
    "    model_names=model_name,\n",
    "    transforms=preprocessor,\n",
    "    prediction_column_prefix=\"preds\",\n",
    "    slice_spec=slice_spec,\n",
    "    batch_size=-1,\n",
    "    fairness_config=fairness_config,\n",
    "    override_fairness_metrics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the ``ClassificationPlotter`` to plot the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyclops.report.plot.classification import ClassificationPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = ClassificationPlotter(task_type=\"binary\", class_names=[\"0\", \"1\"])\n",
    "plotter.set_template(\"plotly_white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the ROC curves and AUROC results for all the slices\n",
    "model_name = f\"model_for_preds.{model_name}\"\n",
    "roc_curves = {\n",
    "    slice_name: slice_results[\"BinaryROC\"]\n",
    "    for slice_name, slice_results in results[model_name].items()\n",
    "}\n",
    "aurocs = {\n",
    "    slice_name: slice_results[\"BinaryAUROC\"]\n",
    "    for slice_name, slice_results in results[model_name].items()\n",
    "}\n",
    "roc_curves.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the ROC curves for all the slices\n",
    "roc_plot = plotter.roc_curve_comparison(roc_curves, aurocs=aurocs)\n",
    "roc_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the metric values for all the slices.\n",
    "slice_metrics = {\n",
    "    slice_name: {\n",
    "        metric_name: metric_value\n",
    "        for metric_name, metric_value in slice_results.items()\n",
    "        if metric_name not in [\"BinaryROC\", \"BinaryPrecisionRecallCurve\"]\n",
    "    }\n",
    "    for slice_name, slice_results in results[model_name].items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metric values for all the slices.\n",
    "slice_metrics_plot = plotter.metrics_comparison_bar(slice_metrics)\n",
    "slice_metrics_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the fairness results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting the fairness metrics\n",
    "fairness_results = copy.deepcopy(results[\"fairness\"])\n",
    "fairness_metrics = {}\n",
    "# remove the group size from the fairness results and add it to the slice name\n",
    "for slice_name, slice_results in fairness_results.items():\n",
    "    group_size = slice_results.pop(\"Group Size\")\n",
    "    fairness_metrics[f\"{slice_name} (Size={group_size})\"] = slice_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the fairness metrics\n",
    "fairness_plot = plotter.metrics_comparison_scatter(\n",
    "    fairness_metrics,\n",
    "    title=\"Fairness Metrics\",\n",
    ")\n",
    "fairness_plot.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
