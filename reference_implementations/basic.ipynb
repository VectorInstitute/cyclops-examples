{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVqJ-vqDTz-x"
      },
      "source": [
        "# Exercise 01 - Getting Started\n",
        "\n",
        "Welcome to the first hands-on exercise to get started with CyclOps!\n",
        "\n",
        "We will go over the installation of CyclOps and introduce the core packages and APIs in this exercise. At the end of this exercise, you will be able to:\n",
        "\n",
        "1. Install the Python package for CyclOps\n",
        "2. Understand the core packages within CyclOps and their purpose\n",
        "3. Learn how to go over the CyclOps API documentation and tutorials\n",
        "4. Explore an example clinical dataset, and understand an important clinical predictive task which we will use in later exercises, where ``CyclOps`` APIs will be leveraged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8HS_ZKATz-0",
        "tags": []
      },
      "source": [
        "## Step 01 - Install CyclOps\n",
        "\n",
        "CyclOps is available as a [Python package](https://pypi.org/project/pycyclops/) and can be installed using ``pip``. Note that we now install ``CyclOps`` with an extra dependency ``xgboost`` since we will be using the [xgboost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) library.\n",
        "\n",
        "``Colab`` will ask you to restart the session, which is normal. Click on ``Restart Session`` and re-run the cell to install ``CyclOps``.\n",
        "\n",
        "**NOTE**: We uninstall ``cupy`` from the colab runtime to avoid conflicts with ``CyclOps`` which would attempt to use ``cupy`` if it is installed. Since the runtime does not support GPUs, we will uninstall ``cupy``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqFekacnTz-1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip uninstall cupy-cuda12x -y\n",
        "!pip install 'pycyclops[xgboost]'\n",
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnMTIGeZ_TzF"
      },
      "source": [
        "## Step 02 - ``CyclOps`` core packages\n",
        "\n",
        "``CyclOps`` has a few core packages that support functionality used for evaluation and monitoring. We will learn a bit more about the packages which we will use in today's workshop.\n",
        "\n",
        "1. ``data`` - The ``data`` package supports loading and processing data into features for your ML model. More importantly, it supports slicing of data across sub-groups which can be pretty useful for evaluating your ML model across patient subpopulations.\n",
        "2. ``evaluate`` - The ``evaluate`` package supports evaluation of your ML model. The package contains a ``metrics`` sub-package which supports common ML performance metrics such as ``Accuracy``, ``Sensitivity`` and ``Specificity``. Furthermore, the ``evaluate`` package also allows calculating fairness metrics which can be used to compare performance of sub-groups with respect to a reference group.\n",
        "3. ``report`` - The ``report`` package supports the creation of model monitoring reports. The package allows users to customize the reports to their use case.\n",
        "\n",
        "There are a few packages that support ML model development and benchmarking:\n",
        "\n",
        "4. ``models`` - The ``models`` package contains baseline model implementations\n",
        "using `scikit-learn`, `xgboost` and `pytorch` libraries. The package allows the user to easily train and evaluate models.\n",
        "5. ``tasks`` - The ``tasks`` package contains a few classes that implement classification tasks. These can be used for classification using tabular or image data, and are used to demonstrate example use cases.\n",
        "6. ``utils`` - The ``utils`` package contains useful utility functions for logging, development, saving and loading data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ngV6mqv_XFc"
      },
      "outputs": [],
      "source": [
        "import pkgutil\n",
        "import cyclops\n",
        "\n",
        "for package in pkgutil.iter_modules(cyclops.__path__):\n",
        "    print(package.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SdbLK6XyDU0"
      },
      "source": [
        "## Step 03 - Explore CyclOps user guide, API documentation and tutorials\n",
        "\n",
        "CyclOps provides detailed documentation available through the github repository. Simply click on the [landing page URL](https://vectorinstitute.github.io/cyclops/). From the landing page, you can navigate to the [API documentation](https://vectorinstitute.github.io/cyclops/api/).\n",
        "\n",
        "The API documentation starts with user guides that cover:\n",
        "\n",
        "1. Installation\n",
        "2. Evaluation\n",
        "3. Model Report\n",
        "4. Monitoring\n",
        "\n",
        "We will be covering all of the above tasks in today's workshop, however you can refer to the user guides when you wish to use CyclOps on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtVfRbDOmR2I"
      },
      "source": [
        "## Step 04 - Explore [Diabetes 130-US Hospitals for Years 1999-2008](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) dataset\n",
        "\n",
        "The [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/) provides several public datasets for research. They also provide a handy python package called [ucimlrepo](https://github.com/uci-ml-repo/ucimlrepo) for downloading datasets. We already installed this package, and now will use it to fetch the [Diabetes 130-US Hospitals for Years 1999-2008](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly58W4d3Tz-8"
      },
      "source": [
        "### Readmission prediction\n",
        "\n",
        "The dataset represents ten years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria:\n",
        "\n",
        "1. It is an inpatient encounter (a hospital admission).\n",
        "2. It is a diabetic encounter, that is, one during which any kind of diabetes was entered into the system as a diagnosis.\n",
        "3. The length of stay was at least 1 day and at most 14 days.\n",
        "4. Laboratory tests were performed during the encounter.\n",
        "5. Medications were administered during the encounter.\n",
        "\n",
        "It also contains ``Days to inpatient readmission``. Values: ``<30`` if the patient was readmitted in less than 30 days, ``>30`` if the patient was readmitted in more than 30 days, and ``No`` for no record of readmission.\n",
        "\n",
        "Using this information we could predict early readmission of the patient within 30 days of discharge. This problem is important for the following reasons:\n",
        "\n",
        "1. Despite high-quality evidence showing improved clinical outcomes for diabetic patients who receive various preventive and therapeutic interventions, many patients do not receive them. This can be partially attributed to arbitrary diabetes management in hospital environments, which fail to attend to glycemic control.\n",
        "2. Failure to provide proper diabetes care not only increases the managing costs for the hospitals (as the patients are readmitted) but also impacts the morbidity and mortality of the patients, who may face complications associated with diabetes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwNbWvsTxNXg"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "diabetes_130_data = fetch_ucirepo(\n",
        "    id=296\n",
        ")  # This ID specifically corresponds to the Diabetes 130 dataset\n",
        "features = diabetes_130_data[\"data\"][\"features\"]\n",
        "targets = diabetes_130_data[\"data\"][\"targets\"]\n",
        "metadata = diabetes_130_data[\"metadata\"]\n",
        "variables = diabetes_130_data[\"variables\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXIyF3qTxtpy"
      },
      "outputs": [],
      "source": [
        "metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOquKCTF0I4X"
      },
      "source": [
        "Let's visualize the distribution of the data with respect to a few key variables such as Age, Gender and the prediction outcome of interest. We will use the popular [plotly](https://plotly.com/python/) library to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVeTu-Me0aEg"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jouJcBTETz-6"
      },
      "source": [
        "### Distribution across gender\n",
        "\n",
        "We see a pretty balanced distribution across Male and Female genders. There is a small number of samples which seem to have missing/invalid values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkoG09teTz-7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig = px.pie(features, names=\"gender\")\n",
        "fig.update_layout(\n",
        "    title=\"Gender Distribution\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UmB-E_jTz-7"
      },
      "source": [
        "###  Distribution across age\n",
        "\n",
        "We see a slightly skewed normal distribution across age brackets. The majority of the patients are in the 70-80 age group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkk_7RDNTz-7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(features, x=\"age\")\n",
        "fig.update_layout(\n",
        "    title=\"Age Distribution\",\n",
        "    xaxis_title=\"Age\",\n",
        "    yaxis_title=\"Count\",\n",
        "    bargap=0.2,\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiZarTLW4JWu"
      },
      "source": [
        "###  Distribution across race\n",
        "\n",
        "We see a very unbalanced distribution across races. We have very few samples for Asian and Hispanic populations. This distribution is partially indicative of the patient population and hence the demographics of the region. However, it is also indicative of a bias in the data which could stem from socio-demographic inequalities (i.e. access to healthcare)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgNN5ZIw4L1o"
      },
      "outputs": [],
      "source": [
        "fig = px.histogram(features, x=\"race\")\n",
        "fig.update_layout(\n",
        "    title=\"Race Distribution\",\n",
        "    xaxis_title=\"Race\",\n",
        "    yaxis_title=\"Count\",\n",
        "    bargap=0.2,\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhgeUt2un1K_"
      },
      "source": [
        "### Missing values\n",
        "\n",
        "Let's see how much missing data there is, and which variables have the most missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0hkNpuUn1K_"
      },
      "outputs": [],
      "source": [
        "null_counts = features.isnull().sum()[features.isnull().sum() > 0]\n",
        "fig = go.Figure(data=[go.Bar(x=null_counts.index, y=null_counts.values)])\n",
        "fig.update_layout(\n",
        "    title=\"Number of Null Values per Column\",\n",
        "    xaxis_title=\"Columns\",\n",
        "    yaxis_title=\"Number of Null Values\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LghEK1Gnn1K_"
      },
      "source": [
        "###  Distribution across outcome (readmission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX2fq1ncTz-8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig = px.pie(targets, names=\"readmitted\")\n",
        "fig.update_traces(textinfo=\"percent+label\")\n",
        "fig.update_layout(title_text=\"Outcome Distribution\")\n",
        "fig.update_traces(\n",
        "    hovertemplate=\"Outcome: %{label}<br>Count: \\\n",
        "    %{value}<br>Percent: %{percent}\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZCBd0aOn1K_"
      },
      "source": [
        "That's the end of the first exercise!\n",
        "\n",
        "A summary of what we learnt:\n",
        "1. Installed the CyclOps Python package\n",
        "2. Learnt about the core packages within CyclOps\n",
        "3. Learnt about where to find the API documentation and tutorials\n",
        "4. Explored a clinical dataset to understand the distribution of data across variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhtm1CQBn1LA"
      },
      "source": [
        "# Exercise 02 - Training an ML model\n",
        "\n",
        "Welcome to the second hands-on exercise!\n",
        "\n",
        "We will use the dataset introduced in the first exercise to train an ML model! At the end of this exercise, you will be able to:\n",
        "\n",
        "1. Create training and validation datasets using the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library\n",
        "2. Train a baseline ML model using CyclOps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip4MdMnUn1LA"
      },
      "source": [
        "First, we will transform the readmitted variable into binary 0/1 labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrBtl7Cin1LA"
      },
      "outputs": [],
      "source": [
        "def transform_label(value):\n",
        "    \"\"\"Transform string labels of readmission into 0/1 binary labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    value: str\n",
        "        Input value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        0 if not readmitted or if greater than 30 days, 1 if less than 30 days\n",
        "\n",
        "    \"\"\"\n",
        "    if value in [\"NO\", \">30\"]:\n",
        "        return 0\n",
        "    if value == \"<30\":\n",
        "        return 1\n",
        "\n",
        "    raise ValueError(\"Unexpected value for readmission!\")\n",
        "\n",
        "\n",
        "df = features\n",
        "targets.loc[:, \"readmitted\"] = targets[\"readmitted\"].apply(transform_label)\n",
        "df.loc[:, \"readmitted\"] = targets[\"readmitted\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj9O57HMn1LA"
      },
      "source": [
        "Due to the large size of the dataset (around 100k examples), we will choose a small subset for training an ML model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HDMpJ7Zn1LA"
      },
      "outputs": [],
      "source": [
        "df = df[0:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnGamECXn1LB"
      },
      "source": [
        "We previously looked at the missingness in the data. Let's remove features that are NaNs or have just a single unique value!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT7wf7nan1LB"
      },
      "outputs": [],
      "source": [
        "features_to_remove = []\n",
        "for col in df:\n",
        "    if len(df[col].value_counts()) <= 1:\n",
        "        features_to_remove.append(col)\n",
        "df = df.drop(columns=features_to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hr2lN9An1LB"
      },
      "source": [
        "It is also important that we understand the class imbalance and use it to train our binary classifier to weight the class with fewer examples accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSPzISVJn1LB"
      },
      "outputs": [],
      "source": [
        "class_counts = df[\"readmitted\"].value_counts()\n",
        "class_ratio = class_counts[0] / class_counts[1]\n",
        "print(class_ratio, class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ0GfsFun1LC"
      },
      "source": [
        "From the features in the dataset, we select all of them except the label to train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7FvoI4wn1LC"
      },
      "outputs": [],
      "source": [
        "features_list = list(df.columns)\n",
        "features_list.remove(\"readmitted\")\n",
        "features_list = sorted(features_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjepI5Sxn1LC"
      },
      "source": [
        "### Identifying feature types\n",
        "\n",
        "Cyclops `TabularFeatures` class helps to identify feature types, an essential step before preprocessing the data. Understanding feature types (numerical/categorical/binary) allows us to apply appropriate preprocessing steps for each type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63PvpPGwn1LC"
      },
      "outputs": [],
      "source": [
        "from cyclops.data.df.feature import TabularFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdX5RNatn1LC"
      },
      "outputs": [],
      "source": [
        "tab_features = TabularFeatures(\n",
        "    data=df.reset_index(),\n",
        "    features=features_list,\n",
        "    by=\"index\",\n",
        "    targets=\"readmitted\",\n",
        ")\n",
        "print(tab_features.types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cibDGRmin1LD"
      },
      "source": [
        "### Creating data preprocessors\n",
        "\n",
        "We create a data preprocessor using sklearn's ColumnTransformer. This helps in applying different preprocessing steps to different columns in the dataframe. For instance, binary features might be processed differently from numeric features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sx4_J2un1LD"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph9iZ-0Dn1LJ"
      },
      "outputs": [],
      "source": [
        "numeric_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())],\n",
        ")\n",
        "\n",
        "binary_transformer = Pipeline(\n",
        "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLCRc-NTn1LJ"
      },
      "outputs": [],
      "source": [
        "numeric_features = sorted((tab_features.features_by_type(\"numeric\")))\n",
        "numeric_indices = [\n",
        "    df[features_list].columns.get_loc(column) for column in numeric_features\n",
        "]\n",
        "print(numeric_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViUma4idn1LK"
      },
      "outputs": [],
      "source": [
        "binary_features = sorted(tab_features.features_by_type(\"binary\"))\n",
        "binary_features.remove(\"readmitted\")\n",
        "ordinal_features = sorted(\n",
        "    tab_features.features_by_type(\"ordinal\")\n",
        "    + [\"medical_specialty\", \"diag_1\", \"diag_2\", \"diag_3\"]\n",
        ")\n",
        "binary_indices = [\n",
        "    df[features_list].columns.get_loc(column) for column in binary_features\n",
        "]\n",
        "ordinal_indices = [\n",
        "    df[features_list].columns.get_loc(column) for column in ordinal_features\n",
        "]\n",
        "print(binary_features, ordinal_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODr3MBXun1LK"
      },
      "outputs": [],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_indices),\n",
        "        (\n",
        "            \"onehot\",\n",
        "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
        "            binary_indices + ordinal_indices,\n",
        "        ),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8wFqUHXn1LK"
      },
      "source": [
        "### Creating Hugging Face Dataset\n",
        "\n",
        "We convert our processed Pandas dataframe into a Hugging Face dataset, a powerful and easy-to-use data format which is also compatible with CyclOps modules. The dataset is then split into train and test sets (80:20 split)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prvUocx1n1LK"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from datasets.features import ClassLabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWuXFV8qn1LK"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 85\n",
        "TRAIN_SIZE = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOpYo2mgn1LL"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "dataset.cleanup_cache_files()\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV-k8F9Tn1LL"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.cast_column(\"readmitted\", ClassLabel(num_classes=2))\n",
        "dataset = dataset.train_test_split(\n",
        "    train_size=TRAIN_SIZE,\n",
        "    stratify_by_column=\"readmitted\",\n",
        "    seed=RANDOM_SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhZE-zmzn1LL"
      },
      "source": [
        "## Step 03 - Create model and train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-N1GLELn1LL"
      },
      "source": [
        "### Model Creation\n",
        "\n",
        "CyclOps model registry allows for straightforward creation and selection of models. This registry maintains a list of pre-configured models, which can be instantiated with a single line of code. Here we use a [XGBoost classifier](https://xgboost.readthedocs.io/en/stable/python/python_api.html) to fit a binary classification model. The model configurations can be passed to `create_model` based on the parameters for the ``XGBClassifier``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq5BK0IBn1LL"
      },
      "outputs": [],
      "source": [
        "from cyclops.models.catalog import create_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEno9HD4n1LM"
      },
      "outputs": [],
      "source": [
        "model_name = \"xgb_classifier\"\n",
        "model = create_model(model_name, random_state=123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVy5S0m7n1LM"
      },
      "source": [
        "### Task Creation\n",
        "\n",
        "We use Cyclops tasks to define our model's task (in this case, readmission prediction), train the model, make predictions, and evaluate performance. Cyclops task classes encapsulate the entire ML pipeline into a single, cohesive structure, making the process smooth and easy to manage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ2bUPeXo8Hk"
      },
      "outputs": [],
      "source": [
        "from cyclops.tasks import BinaryTabularClassificationTask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LooIsJBhn1LM"
      },
      "outputs": [],
      "source": [
        "readmission_prediction_task = BinaryTabularClassificationTask(\n",
        "    {model_name: model},\n",
        "    task_features=features_list,\n",
        "    task_target=\"readmitted\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnUcI27Vn1LM"
      },
      "outputs": [],
      "source": [
        "readmission_prediction_task.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMeg9GX9n1LM"
      },
      "source": [
        "### Training\n",
        "\n",
        "If `best_model_params` is passed to the `train` method, the best model will be selected after the hyperparameter search. The parameters in `best_model_params` indicate the values to create the parameters grid.\n",
        "\n",
        "Note that the data preprocessor needs to be passed to the tasks methods if the Hugging Face dataset is not already preprocessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-sBO1BSn1LN"
      },
      "outputs": [],
      "source": [
        "best_model_params = {\n",
        "    \"n_estimators\": [250, 500],\n",
        "    \"learning_rate\": [0.1],\n",
        "    \"max_depth\": [5],\n",
        "    \"reg_lambda\": [0, 1, 10],\n",
        "    \"colsample_bytree\": [0.8],\n",
        "    \"gamma\": [0, 1],\n",
        "    \"method\": \"random\",\n",
        "    \"scale_pos_weight\": [int(class_ratio)],\n",
        "}\n",
        "readmission_prediction_task.train(\n",
        "    dataset[\"train\"],\n",
        "    model_name=model_name,\n",
        "    transforms=preprocessor,\n",
        "    best_model_params=best_model_params,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WRXYzm3n1LN"
      },
      "source": [
        "This is the end of the second exercise!\n",
        "\n",
        "A summary of what we learnt:\n",
        "1. Created training and validation datasets using the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library\n",
        "2. Trained a baseline ML model using CyclOps"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}